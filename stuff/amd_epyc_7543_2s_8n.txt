          0.5557,  0.6667,  0.4628,  0.6486,  0.5385,  0.6588,  0.4645,  0.4984,
          0.4415,  0.5065,  0.3765,  0.4258,  0.3312,  0.2856,  0.2606,  0.5957,
          0.4213,  0.4688,  0.5620, -0.2119,  0.5868,  0.6878,  0.6056,  0.7054,
          0.5540,  0.6809,  0.5451,  0.6771,  0.6436,  0.7017,  0.6591,  0.6390],
        [ 0.5163,  0.5753,  0.3938,  0.6200,  0.5641,  0.6134,  0.4988,  0.7165,
          0.4745,  0.6409,  0.4420,  0.6124,  0.5084,  0.3415,  0.5938,  0.5435,
          0.3626,  0.2858,  0.4013,  0.3376,  0.4156,  0.5352,  0.4943,  0.4791,
          0.2645, -0.9017,  0.6065,  0.5377,  0.5941,  0.5751,  0.6010,  0.7270,
          0.5659,  0.7089,  0.4893,  0.6472,  0.5557,  0.6462,  0.4557,  0.5362,
          0.4640,  0.5040,  0.3653,  0.4309,  0.3488,  0.2538,  0.2782,  0.6054,
          0.4349,  0.4761,  0.5669, -0.2483,  0.5904,  0.6637,  0.5782,  0.6985,
          0.5693,  0.6777,  0.5417,  0.6490,  0.6627,  0.6649,  0.6481,  0.6244],
        [ 0.5235,  0.5727,  0.4015,  0.6143,  0.5455,  0.5904,  0.5139,  0.7022,
          0.5270,  0.6182,  0.4585,  0.5634,  0.4657,  0.3559,  0.5581,  0.5124,
          0.3349,  0.3188,  0.4047,  0.3262,  0.3647,  0.5614,  0.4957,  0.4825,
          0.2392, -0.9481,  0.5768,  0.5010,  0.6129,  0.5793,  0.6206,  0.6961,
          0.5431,  0.6720,  0.4477,  0.6099,  0.5283,  0.6280,  0.4127,  0.4839,
          0.4411,  0.4814,  0.3599,  0.4283,  0.3152,  0.2910,  0.2461,  0.5670,
          0.4311,  0.4895,  0.5790, -0.2360,  0.5817,  0.6454,  0.5922,  0.7032,
          0.5456,  0.7093,  0.5163,  0.6249,  0.6375,  0.7009,  0.6770,  0.6346],
        [ 0.5158,  0.5565,  0.4259,  0.6247,  0.5855,  0.6051,  0.5203,  0.6916,
          0.5105,  0.6089,  0.4887,  0.6247,  0.5140,  0.3734,  0.5690,  0.5072,
          0.3914,  0.3307,  0.4116,  0.3112,  0.4219,  0.5393,  0.5058,  0.4972,
          0.2462, -0.9263,  0.5533,  0.5033,  0.6040,  0.5908,  0.6222,  0.7127,
          0.5626,  0.6568,  0.4893,  0.6305,  0.5177,  0.6326,  0.4660,  0.5114,
          0.4440,  0.5215,  0.4140,  0.3996,  0.3299,  0.3023,  0.2563,  0.5683,
          0.4116,  0.5080,  0.5832, -0.2403,  0.6150,  0.6707,  0.5989,  0.7068,
          0.5545,  0.6901,  0.5577,  0.6679,  0.6501,  0.7004,  0.6660,  0.6676],
        [ 0.5593,  0.5762,  0.3718,  0.6117,  0.5770,  0.6080,  0.5039,  0.7283,
          0.5195,  0.6027,  0.4516,  0.5892,  0.5282,  0.3393,  0.5664,  0.5067,
          0.3730,  0.2885,  0.4030,  0.3427,  0.4201,  0.5466,  0.4713,  0.4815,
          0.2520, -0.9206,  0.5981,  0.5272,  0.6361,  0.5944,  0.6516,  0.6952,
          0.5170,  0.6161,  0.4752,  0.6445,  0.5318,  0.6323,  0.4600,  0.4835,
          0.4282,  0.4975,  0.3838,  0.4435,  0.3168,  0.3255,  0.2582,  0.5666,
          0.4357,  0.4868,  0.5637, -0.2434,  0.5932,  0.6949,  0.5992,  0.6936,
          0.5358,  0.6946,  0.5515,  0.6629,  0.6601,  0.7032,  0.6786,  0.6437],
        [ 0.5342,  0.5814,  0.3909,  0.6005,  0.5685,  0.5773,  0.5094,  0.7060,
          0.5244,  0.6181,  0.4344,  0.5987,  0.5139,  0.3222,  0.5836,  0.5093,
          0.3734,  0.3217,  0.3837,  0.3389,  0.4211,  0.5265,  0.5142,  0.5331,
          0.2382, -0.8956,  0.6117,  0.4782,  0.5911,  0.5821,  0.5994,  0.6996,
          0.5326,  0.6564,  0.4726,  0.6099,  0.5203,  0.6459,  0.4631,  0.5162,
          0.4470,  0.4986,  0.3764,  0.4076,  0.3430,  0.2406,  0.2505,  0.5845,
          0.4600,  0.5164,  0.5695, -0.2149,  0.5645,  0.6509,  0.5938,  0.6925,
          0.5523,  0.6786,  0.5722,  0.6506,  0.6706,  0.6691,  0.6876,  0.6404],
        [ 0.5256,  0.5491,  0.4151,  0.6234,  0.6249,  0.5643,  0.4915,  0.7300,
          0.4864,  0.5741,  0.4530,  0.5987,  0.5034,  0.4051,  0.6021,  0.5285,
          0.3521,  0.3292,  0.4022,  0.3080,  0.4254,  0.5535,  0.5126,  0.5424,
          0.2258, -0.9125,  0.5815,  0.5086,  0.6326,  0.5802,  0.5704,  0.7076,
          0.5707,  0.6499,  0.4812,  0.6167,  0.5043,  0.6228,  0.5022,  0.5056,
          0.4611,  0.5518,  0.4192,  0.4009,  0.3315,  0.3690,  0.2565,  0.6314,
          0.4416,  0.4852,  0.5204, -0.2570,  0.6050,  0.7029,  0.5966,  0.7058,
          0.5119,  0.6822,  0.5278,  0.6905,  0.6678,  0.6940,  0.6668,  0.5646],
        [ 0.5374,  0.5624,  0.3981,  0.5970,  0.5735,  0.6079,  0.4951,  0.7111,
          0.5116,  0.5834,  0.4648,  0.5819,  0.5123,  0.3559,  0.5291,  0.5063,
          0.3492,  0.3386,  0.4190,  0.2892,  0.4082,  0.5398,  0.4703,  0.4688,
          0.2376, -0.9320,  0.5819,  0.4996,  0.5938,  0.6068,  0.6297,  0.7082,
          0.5560,  0.6730,  0.4875,  0.6254,  0.5229,  0.6326,  0.4325,  0.4726,
          0.4583,  0.4780,  0.3702,  0.4193,  0.3141,  0.2739,  0.2504,  0.5616,
          0.4336,  0.5241,  0.5779, -0.2389,  0.5815,  0.6574,  0.5880,  0.6918,
          0.5349,  0.6964,  0.5728,  0.6354,  0.6769,  0.6812,  0.6613,  0.6773],
        [ 0.5208,  0.5997,  0.3908,  0.5893,  0.5497,  0.5738,  0.4939,  0.7276,
          0.5602,  0.5801,  0.4670,  0.5700,  0.5437,  0.3455,  0.5826,  0.5305,
          0.3299,  0.3325,  0.4218,  0.2807,  0.3485,  0.5189,  0.4771,  0.4903,
          0.2384, -0.9224,  0.6126,  0.5695,  0.5932,  0.5620,  0.6592,  0.7153,
          0.5610,  0.6806,  0.4972,  0.6506,  0.5440,  0.5935,  0.4416,  0.5080,
          0.4441,  0.5120,  0.3644,  0.4516,  0.3048,  0.2693,  0.2678,  0.5926,
          0.4378,  0.5012,  0.5693, -0.2165,  0.5769,  0.6558,  0.5971,  0.6695,
          0.5338,  0.6367,  0.5667,  0.6350,  0.6693,  0.6811,  0.6829,  0.6525],
        [ 0.5462,  0.5636,  0.4060,  0.6191,  0.5857,  0.6253,  0.5169,  0.6787,
          0.5349,  0.5630,  0.4705,  0.5902,  0.5105,  0.3536,  0.5370,  0.5106,
          0.3316,  0.3494,  0.4172,  0.3254,  0.4010,  0.5132,  0.4798,  0.4989,
          0.2314, -0.9273,  0.5888,  0.4892,  0.5862,  0.5605,  0.6117,  0.6614,
          0.5689,  0.6758,  0.4758,  0.6349,  0.5511,  0.6619,  0.4949,  0.5089,
          0.4624,  0.5186,  0.3781,  0.4460,  0.3484,  0.2612,  0.2522,  0.5912,
          0.4384,  0.5470,  0.5693, -0.2024,  0.5565,  0.6651,  0.6196,  0.6753,
          0.5417,  0.6973,  0.5770,  0.6868,  0.6727,  0.7196,  0.6515,  0.6705],
        [ 0.5187,  0.5812,  0.4084,  0.6290,  0.5739,  0.6179,  0.5072,  0.6957,
          0.5187,  0.6152,  0.4671,  0.6166,  0.4994,  0.3513,  0.5711,  0.5166,
          0.3517,  0.3344,  0.4001,  0.3405,  0.3954,  0.5548,  0.4691,  0.4713,
          0.2441, -0.9160,  0.5859,  0.4830,  0.5750,  0.5925,  0.6030,  0.6979,
          0.5442,  0.6725,  0.4989,  0.6377,  0.5016,  0.6597,  0.4559,  0.5118,
          0.4784,  0.5064,  0.4106,  0.4197,  0.3357,  0.2827,  0.2294,  0.5688,
          0.4591,  0.4932,  0.5990, -0.2177,  0.6010,  0.6717,  0.6026,  0.7070,
          0.5420,  0.7304,  0.5448,  0.6687,  0.6506,  0.6945,  0.6881,  0.6554],
        [ 0.4986,  0.5827,  0.4255,  0.6209,  0.5816,  0.6280,  0.4894,  0.6997,
          0.5102,  0.6213,  0.4742,  0.6185,  0.5184,  0.3615,  0.5555,  0.5026,
          0.3532,  0.3184,  0.4046,  0.3042,  0.4199,  0.5293,  0.5134,  0.4935,
          0.2574, -0.8967,  0.6006,  0.5099,  0.6087,  0.5922,  0.6246,  0.6985,
          0.5401,  0.6598,  0.4956,  0.6354,  0.5256,  0.6423,  0.4738,  0.5125,
          0.4628,  0.5437,  0.3859,  0.3962,  0.3479,  0.2508,  0.2686,  0.5686,
          0.4090,  0.4924,  0.5800, -0.2294,  0.5994,  0.6644,  0.5961,  0.7285,
          0.5578,  0.7359,  0.5895,  0.6800,  0.6370,  0.6993,  0.6624,  0.6661],
        [ 0.5733,  0.5845,  0.4154,  0.6243,  0.6024,  0.5798,  0.5078,  0.7090,
          0.4936,  0.6396,  0.4300,  0.5921,  0.5285,  0.3494,  0.5905,  0.5242,
          0.3653,  0.3058,  0.3738,  0.3445,  0.3585,  0.4975,  0.4415,  0.4501,
          0.2807, -0.9186,  0.6285,  0.4829,  0.6667,  0.5940,  0.6444,  0.6716,
          0.5436,  0.6271,  0.4488,  0.6311,  0.5103,  0.6095,  0.4401,  0.4667,
          0.4859,  0.5162,  0.3605,  0.4242,  0.3506,  0.3008,  0.2324,  0.5813,
          0.4177,  0.4979,  0.6021, -0.2374,  0.6239,  0.6470,  0.5802,  0.7088,
          0.5490,  0.6816,  0.5945,  0.6619,  0.6598,  0.7107,  0.6434,  0.6549],
        [ 0.5299,  0.5760,  0.3826,  0.6178,  0.5648,  0.5951,  0.4931,  0.6919,
          0.5100,  0.6135,  0.4443,  0.5840,  0.4980,  0.3548,  0.5757,  0.5239,
          0.3580,  0.3084,  0.3819,  0.3220,  0.3853,  0.5477,  0.4249,  0.4719,
          0.2430, -0.9200,  0.6170,  0.5076,  0.6494,  0.5724,  0.6338,  0.6867,
          0.5070,  0.6684,  0.5020,  0.6493,  0.5413,  0.6332,  0.4591,  0.4889,
          0.4336,  0.5045,  0.3688,  0.4558,  0.3247,  0.2928,  0.2774,  0.6058,
          0.4098,  0.4819,  0.5587, -0.2496,  0.5730,  0.6738,  0.5767,  0.7367,
          0.5271,  0.7096,  0.5856,  0.6467,  0.6287,  0.6860,  0.6532,  0.6343],
        [ 0.5136,  0.5841,  0.3993,  0.6121,  0.5657,  0.6187,  0.4760,  0.7086,
          0.4928,  0.6206,  0.4197,  0.6009,  0.5198,  0.3857,  0.5737,  0.5339,
          0.3506,  0.3130,  0.4117,  0.3196,  0.4208,  0.5599,  0.4912,  0.4758,
          0.2434, -0.8933,  0.6037,  0.5068,  0.5873,  0.5716,  0.6291,  0.7052,
          0.5715,  0.6531,  0.4665,  0.6242,  0.5358,  0.6437,  0.4249,  0.5036,
          0.4763,  0.4840,  0.3687,  0.4255,  0.3465,  0.2130,  0.2514,  0.5691,
          0.4542,  0.5058,  0.5684, -0.2411,  0.5843,  0.6581,  0.5808,  0.6885,
          0.5288,  0.7116,  0.5905,  0.6420,  0.6400,  0.6867,  0.6755,  0.6320],
        [ 0.5581,  0.5866,  0.4212,  0.6269,  0.5558,  0.6028,  0.4908,  0.6798,
          0.5001,  0.6234,  0.4228,  0.5842,  0.5259,  0.3388,  0.5452,  0.5100,
          0.3340,  0.3482,  0.4211,  0.3537,  0.3943,  0.5247,  0.4280,  0.4505,
          0.2789, -0.9067,  0.5783,  0.5054,  0.6034,  0.6053,  0.6437,  0.6764,
          0.5549,  0.6587,  0.4698,  0.6382,  0.4983,  0.6206,  0.4275,  0.4928,
          0.5119,  0.5038,  0.3637,  0.4269,  0.3447,  0.2666,  0.2493,  0.5555,
          0.4434,  0.5200,  0.5993, -0.2563,  0.6053,  0.6596,  0.6082,  0.7004,
          0.5310,  0.7182,  0.5855,  0.6403,  0.6706,  0.6815,  0.6819,  0.6888],
        [ 0.5389,  0.5640,  0.4373,  0.6643,  0.5636,  0.6100,  0.5020,  0.6827,
          0.5240,  0.6170,  0.4272,  0.5857,  0.4759,  0.4024,  0.5783,  0.5313,
          0.3343,  0.3464,  0.4361,  0.3384,  0.3991,  0.5088,  0.4544,  0.4997,
          0.2552, -0.9095,  0.5595,  0.5162,  0.5838,  0.5608,  0.6377,  0.7160,
          0.5871,  0.6538,  0.4703,  0.6259,  0.5360,  0.6044,  0.4335,  0.4569,
          0.4549,  0.4884,  0.3827,  0.4366,  0.3374,  0.2905,  0.2346,  0.5503,
          0.4929,  0.5308,  0.5955, -0.2593,  0.5693,  0.6713,  0.5668,  0.6539,
          0.5413,  0.6535,  0.5788,  0.6902,  0.6610,  0.6849,  0.6854,  0.6527]],
       grad_fn=<SelectBackward0>)
epoch 1 iter 1: train loss 4.15491. lr 5.999882e-04.:   2%|█▋                                                                                              | 2/114 [00:15<14:23,  7.71s/it]
Traceback (most recent call last):
  File "/home/yrayhan/works/lpmoss/run_dt_place.py", line 277, in <module>
    trainer.train()
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 197, in train
    run_epoch('train', epoch_num=epoch)
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 123, in run_epoch
    loss.backward()
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

(idx_creator) yrayhan@dbserver2:~/works/lpmoss$ python run_dt_place.py
ExpConfig(processor=amd_epyc7543_2s_8n
chassis_dim=(8, 8)
index=0
workload=32
num_features=12
num_meta_features=0
cnt_grid_cells=256
cfg_par=4
rtg_scale=1.1, rtg_div=100000
eval_start_cfg=30
idx_kb_folder=kb_b__
machine=Machine(name=amd_epyc7543_2s_8n, cnt_numa_node=8, worker_per_numa=6, mc_channel_per_numa=3, num_worker=48, li_ncore_dumper=[1, 9, 17, 25, 33, 41, 49, 57], li_worker=[2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63], chassis mapping = [16, 24, 1, 9, 17, 25, 18, 26, 3, 11, 19, 27, 48, 56, 33, 41, 49, 57, 50, 58, 35, 43, 51, 59, 20, 28, 5, 13, 21, 29, 52, 60, 37, 45, 53, 61, 22, 30, 7, 15, 23, 31, 54, 62, 39, 47, 55, 63])
kb_path=/home/yrayhan/works/lpmoss/kbs/amd/epyc7543_2s_8n/kb_b__/
MAXCONFIG = 36 MAXIDX= 1616
405387960
(933632,)
(933632, 1, 8, 8)
(933632, 1, 8, 8, 13)
(933632, 1, 8, 8)
(933632, 1)
(933632, 1)
(3647,)
(933632,)
MAXCONFIG = 36 MAXIDX= 1616
405387960
MAXCONFIG = 53 MAXIDX= 3338
267104516
(141568,)
(141568, 1, 8, 8)
(141568, 1, 8, 8, 13)
(141568, 1, 8, 8)
(141568, 1)
(141568, 1)
(553,)
(141568,)
============================================================================================================
create dataset finish.
obss shape =  (933632, 1, 8, 8)
obss_wire shape =  (933632, 1, 8, 8, 13)
obss_mask shape =  (933632, 1, 8, 8)
actions shape =  (933632,)
done_idxs shape =  (3647,)
rtgs shape =  (933632, 1)
timesteps shape =  (933632,)
benchmarks shape =  (933632, 1)
stepwise_returns shape =  (933632, 1)
lengths shape =  (933632, 1)
============================================================================================================
create dataset finish.
data raw shape (933632, 1, 8, 8)
data raw shape (141568, 1, 8, 8)
config.vocab_size 64
10/22/2024 09:04:24 - INFO - mingpt.model_placement -   number of parameters: 1.337600e+06
/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
None
{'Total': 3349307, 'Trainable': 3349307}
trainerconfig finish
trainer build finish
epoch 1 iter 42: train loss 3.68964. lr 5.642268e-04.:  38%|███████████████████████████████████▍                                                          | 43/114 [03:13<05:14,  4.43s/it]epoch 1 iter 42: train loss 3.68964. lr 5.642268e-04.:  38%|███████████████████████████████████▍                                                          | 43/114 [03:18<05:27,  4.61s/it]
Traceback (most recent call last):
  File "/home/yrayhan/works/lpmoss/run_dt_place.py", line 277, in <module>
    trainer.train()
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 197, in train
    run_epoch('train', epoch_num=epoch)
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 123, in run_epoch
    loss.backward()
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

(idx_creator) yrayhan@dbserver2:~/works/lpmoss$ python run_dt_place.py
ExpConfig(processor=amd_epyc7543_2s_8n
chassis_dim=(8, 8)
index=0
workload=32
num_features=12
num_meta_features=0
cnt_grid_cells=256
cfg_par=4
rtg_scale=1.1, rtg_div=100000
eval_start_cfg=30
idx_kb_folder=kb_b__
machine=Machine(name=amd_epyc7543_2s_8n, cnt_numa_node=8, worker_per_numa=6, mc_channel_per_numa=3, num_worker=48, li_ncore_dumper=[1, 9, 17, 25, 33, 41, 49, 57], li_worker=[2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63], chassis mapping = [16, 24, 1, 9, 17, 25, 18, 26, 3, 11, 19, 27, 48, 56, 33, 41, 49, 57, 50, 58, 35, 43, 51, 59, 20, 28, 5, 13, 21, 29, 52, 60, 37, 45, 53, 61, 22, 30, 7, 15, 23, 31, 54, 62, 39, 47, 55, 63])
kb_path=/home/yrayhan/works/lpmoss/kbs/amd/epyc7543_2s_8n/kb_b__/
MAXCONFIG = 36 MAXIDX= 1616
405387960
(933632,)
(933632, 1, 8, 8)
(933632, 1, 8, 8, 13)
(933632, 1, 8, 8)
(933632, 1)
(933632, 1)
(3647,)
(933632,)
MAXCONFIG = 36 MAXIDX= 1616
405387960
MAXCONFIG = 53 MAXIDX= 3338
267104516
Traceback (most recent call last):
  File "/home/yrayhan/works/lpmoss/run_dt_place.py", line 167, in <module>
    = gen_token_for_eval(exp_config)
  File "/home/yrayhan/works/lpmoss/yr_utils.py", line 738, in gen_token_for_eval
    mask_already_full = obs_mask_core.nonzero()
NameError: name 'obs_mask_core' is not defined
(idx_creator) yrayhan@dbserver2:~/works/lpmoss$ python run_dt_place.py
ExpConfig(processor=amd_epyc7543_2s_8n
chassis_dim=(8, 8)
index=0
workload=32
num_features=12
num_meta_features=0
cnt_grid_cells=256
cfg_par=4
rtg_scale=1.1, rtg_div=100000
eval_start_cfg=30
idx_kb_folder=kb_b__
machine=Machine(name=amd_epyc7543_2s_8n, cnt_numa_node=8, worker_per_numa=6, mc_channel_per_numa=3, num_worker=48, li_ncore_dumper=[1, 9, 17, 25, 33, 41, 49, 57], li_worker=[2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63], chassis mapping = [16, 24, 1, 9, 17, 25, 18, 26, 3, 11, 19, 27, 48, 56, 33, 41, 49, 57, 50, 58, 35, 43, 51, 59, 20, 28, 5, 13, 21, 29, 52, 60, 37, 45, 53, 61, 22, 30, 7, 15, 23, 31, 54, 62, 39, 47, 55, 63])
kb_path=/home/yrayhan/works/lpmoss/kbs/amd/epyc7543_2s_8n/kb_b__/
MAXCONFIG = 36 MAXIDX= 1616
405387960
(933632,)
(933632, 1, 8, 8)
(933632, 1, 8, 8, 13)
(933632, 1, 8, 8)
(933632, 1)
(933632, 1)
(3647,)
(933632,)
MAXCONFIG = 36 MAXIDX= 1616
405387960
MAXCONFIG = 53 MAXIDX= 3338
267104516
(141568,)
(141568, 1, 8, 8)
(141568, 1, 8, 8, 13)
(141568, 1, 8, 8)
(141568, 1)
(141568, 1)
(553,)
(141568,)
============================================================================================================
create dataset finish.
obss shape =  (933632, 1, 8, 8)
obss_wire shape =  (933632, 1, 8, 8, 13)
obss_mask shape =  (933632, 1, 8, 8)
actions shape =  (933632,)
done_idxs shape =  (3647,)
rtgs shape =  (933632, 1)
timesteps shape =  (933632,)
benchmarks shape =  (933632, 1)
stepwise_returns shape =  (933632, 1)
lengths shape =  (933632, 1)
============================================================================================================
create dataset finish.
data raw shape (933632, 1, 8, 8)
data raw shape (141568, 1, 8, 8)
config.vocab_size 64
10/22/2024 09:09:38 - INFO - mingpt.model_placement -   number of parameters: 1.337600e+06
/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
None
{'Total': 3349307, 'Trainable': 3349307}
trainerconfig finish
trainer build finish
epoch 1 iter 27: train loss 3.86754. lr 5.851372e-04.:  25%|███████████████████████                                                                       | 28/114 [02:03<06:10,  4.31s/it]epoch 1 iter 27: train loss 3.86754. lr 5.851372e-04.:  25%|███████████████████████                                                                       | 28/114 [02:03<06:20,  4.43s/it]
Traceback (most recent call last):
  File "/home/yrayhan/works/lpmoss/run_dt_place.py", line 277, in <module>
    trainer.train()
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 197, in train
    run_epoch('train', epoch_num=epoch)
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 113, in run_epoch
    logits, loss, acc = model(x, y, y, r, t, m_x, b, st, cir, l)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yrayhan/works/lpmoss/mingpt/model_placement.py", line 368, in forward
    x = self.blocks(x)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yrayhan/works/lpmoss/mingpt/model_placement.py", line 104, in forward
    x = x + self.attn(self.ln1(x))
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yrayhan/works/lpmoss/mingpt/model_placement.py", line 78, in forward
    att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1675, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt

(idx_creator) yrayhan@dbserver2:~/works/lpmoss$ python run_dt_place.py
ExpConfig(processor=amd_epyc7543_2s_8n
chassis_dim=(8, 8)
index=0
workload=32
num_features=12
num_meta_features=0
cnt_grid_cells=256
cfg_par=4
rtg_scale=1.1, rtg_div=100000
eval_start_cfg=30
idx_kb_folder=kb_b__
machine=Machine(name=amd_epyc7543_2s_8n, cnt_numa_node=8, worker_per_numa=6, mc_channel_per_numa=3, num_worker=48, li_ncore_dumper=[1, 9, 17, 25, 33, 41, 49, 57], li_worker=[2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63], chassis mapping = [16, 24, 1, 9, 17, 25, 18, 26, 3, 11, 19, 27, 48, 56, 33, 41, 49, 57, 50, 58, 35, 43, 51, 59, 20, 28, 5, 13, 21, 29, 52, 60, 37, 45, 53, 61, 22, 30, 7, 15, 23, 31, 54, 62, 39, 47, 55, 63])
kb_path=/home/yrayhan/works/lpmoss/kbs/amd/epyc7543_2s_8n/kb_b__/
MAXCONFIG = 36 MAXIDX= 1616
405387960
(933632,)
(933632, 1, 8, 8)
(933632, 1, 8, 8, 13)
(933632, 1, 8, 8)
(933632, 1)
(933632, 1)
(3647,)
(933632,)
MAXCONFIG = 36 MAXIDX= 1616
405387960
MAXCONFIG = 53 MAXIDX= 3338
267104516
(141568,)
(141568, 1, 8, 8)
(141568, 1, 8, 8, 13)
(141568, 1, 8, 8)
(141568, 1)
(141568, 1)
(553,)
(141568,)
============================================================================================================
create dataset finish.
obss shape =  (933632, 1, 8, 8)
obss_wire shape =  (933632, 1, 8, 8, 13)
obss_mask shape =  (933632, 1, 8, 8)
actions shape =  (933632,)
done_idxs shape =  (3647,)
rtgs shape =  (933632, 1)
timesteps shape =  (933632,)
benchmarks shape =  (933632, 1)
stepwise_returns shape =  (933632, 1)
lengths shape =  (933632, 1)
============================================================================================================
create dataset finish.
data raw shape (933632, 1, 8, 8)
data raw shape (141568, 1, 8, 8)
config.vocab_size 64
10/22/2024 09:12:40 - INFO - mingpt.model_placement -   number of parameters: 1.337600e+06
/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
None
{'Total': 3349307, 'Trainable': 3349307}
trainerconfig finish
trainer build finish
epoch 1 iter 35: train loss 3.56832. lr 5.750615e-04.:  31%|████████████████████████████▊                                                      epoch 1 iter 35: train loss 3.56832. lr 5.750615e-04.:  32%|█████████████████████████████▋                                                               epoch 1 iter 36: train loss 3.62586. lr 5.736273e-04.:  32%|█████████████████████████████▋                                                                    epoch 1 iter 36: train loss 3.62586. lr 5.736273e-04.:  32%|██████████████████████████████▌                                                                   epoch 1 iter 37: train loss 3.62502. lr 5.721548e-04.:  32%|██████████████████████████████▌                                                                   epoch 1 iter 37: train loss 3.62502. lr 5.721548e-04.:  33%|███████████████████████████████▎                                                                  epoch 1 iter 38: train loss 3.60380. lr 5.706444e-04.:  33%|███████████████████████████████▎                                                                  epoch 1 iter 38: train loss 3.60380. lr 5.706444e-04.:  34%|████████████████████████████████▏                                                                 epoch 1 iter 39: train loss 3.45966. lr 5.690961e-04.:  34%|████████████████████████████████▏                                                                 epoch 1 iter 39: train loss 3.45966. lr 5.690961e-04.:  35%|████████████████████████████████▉                                                                 epoch 1 iter 40: train loss 3.48453. lr 5.675103e-04.:  35%|████████████████████████████████▉                                                                 epoch 1 iter 40: train loss 3.48453. lr 5.675103e-04.:  36%|█████████████████████████████████▊                                                                epoch 1 iter 41: train loss 3.57742. lr 5.658871e-04.:  36%|█████████████████████████████████▊                                                                epoch 1 iter 41: train loss 3.57742. lr 5.658871e-04.:  37%|██████████████████████████████████▋                                                               epoch 1 iter 42: train loss 3.46013. lr 5.642268e-04.:  37%|██████████████████████████████████▋                                        epoch 1 iter 42: train loss 3.46013. lr 5.642268e-04.:  38%|███████████████████████████████████▍                                                 epoch 1 iter 43: train loss 3.56155. lr 5.625296e-04.:  38%|███████████████████████████████████▍                                                           epoch 1 iter 43: train loss 3.56155. lr 5.625296e-04.:  39%|████████████████████████████████████▎                                                             epoch 1 iter 44: train loss 3.67185. lr 5.607958e-04.:  39%|████████████████████████████████████▎                                                             epoch 1 iter 44: train loss 3.67185. lr 5.607958e-04.:  39%|█████████████████████████████████████                                                             epoch 1 iter 45: train loss 3.43181. lr 5.590255e-04.:  39%|█████████████████████████████████████                                                             epoch 1 iter 45: train loss 3.43181. lr 5.590255e-04.:  40%|█████████████████████████████████████▉                                                            epoch 1 iter 46: train loss 3.37294. lr 5.572191e-04.:  40%|█████████████████████████████████████▉                                                            epoch 1 iter 46: train loss 3.37294. lr 5.572191e-04.:  41%|██████████████████████████████████████▊                                                           epoch 1 iter 47: train loss 3.50683. lr 5.553767e-04.:  41%|██████████████████████████████████████▊                                                           epoch 1 iter 47: train loss 3.50683. lr 5.553767e-04.:  42%|███████████████████████████████████████▌                                                          epoch 1 iter 48: train loss 3.53009. lr 5.534987e-04.:  42%|███████████████████████████████████████▌                                                          epoch 1 iter 48: train loss 3.53009. lr 5.534987e-04.:  43%|████████████████████████████████████████▍                                                         epoch 1 iter 49: train loss 3.42561. lr 5.515853e-04.:  43%|████████████████████████████████████████▍                                                         epoch 1 iter 49: train loss 3.42561. lr 5.515853e-04.:  44%|█████████████████████████████████████████▏                                   epoch 1 iter 50: train loss 3.48974. lr 5.496368e-04.:  44%|█████████████████████████████████████████▏                                             epoch 1 iter 50: train loss 3.48974. lr 5.496368e-04.:  45%|██████████████████████████████████████████                                                       epoch 1 iter 51: train loss 3.55849. lr 5.476534e-04.:  45%|██████████████████████████████████████████                                                        epoch 1 iter 51: train loss 3.55849. lr 5.476534e-04.:  46%|██████████████████████████████████████████▉                                                       epoch 1 iter 52: train loss 3.45922. lr 5.456354e-04.:  46%|██████████████████████████████████████████▉                                                       epoch 1 iter 52: train loss 3.45922. lr 5.456354e-04.:  46%|███████████████████████████████████████████▋                                                      epoch 1 iter 53: train loss 3.29053. lr 5.435831e-04.:  46%|███████████████████████████████████████████▋                                                      epoch 1 iter 53: train loss 3.29053. lr 5.435831e-04.:  47%|████████████████████████████████████████████▌                                                     epoch 1 iter 54: train loss 3.49862. lr 5.414969e-04.:  47%|████████████████████████████████████████████▌                                                     epoch 1 iter 54: train loss 3.49862. lr 5.414969e-04.:  48%|█████████████████████████████████████████████▎                                                    epoch 1 iter 55: train loss 3.39925. lr 5.393769e-04.:  48%|█████████████████████████████████████████████▎                                                    epoch 1 iter 55: train loss 3.39925. lr 5.393769e-04.:  49%|██████████████████████████████████████████████▏                                                   epoch 1 iter 56: train loss 3.46226. lr 5.372235e-04.:  49%|██████████████████████████████████████████████▏                                                   epoch 1 iter 56: train loss 3.46226. lr 5.372235e-04.:  50%|███████████████████████████████████████████████                                                   epoch 1 iter 57: train loss 3.35533. lr 5.350369e-04.:  50%|███████████████████████████████████████████████                                epoch 1 iter 57: train loss 3.35533. lr 5.350369e-04.:  51%|███████████████████████████████████████████████▊                                         epoch 1 iter 58: train loss 3.38910. lr 5.328176e-04.:  51%|███████████████████████████████████████████████▊                                                  epoch 1 iter 58: train loss 3.38910. lr 5.328176e-04.:  52%|████████████████████████████████████████████████▋                                                 epoch 1 iter 59: train loss 3.43459. lr 5.305657e-04.:  52%|████████████████████████████████████████████████▋                                                 epoch 1 iter 59: train loss 3.43459. lr 5.305657e-04.:  53%|█████████████████████████████████████████████████▍                                                epoch 1 iter 60: train loss 3.56757. lr 5.282816e-04.:  53%|█████████████████████████████████████████████████▍                                                epoch 1 iter 60: train loss 3.56757. lr 5.282816e-04.:  54%|██████████████████████████████████████████████████▎                                               epoch 1 iter 61: train loss 3.39384. lr 5.259657e-04.:  54%|██████████████████████████████████████████████████▎                                               epoch 1 iter 61: train loss 3.39384. lr 5.259657e-04.:  54%|███████████████████████████████████████████████████                                               epoch 1 iter 62: train loss 3.48452. lr 5.236182e-04.:  54%|███████████████████████████████████████████████████                                               epoch 1 iter 62: train loss 3.48452. lr 5.236182e-04.:  55%|███████████████████████████████████████████████████▉                                              epoch 1 iter 63: train loss 3.25689. lr 5.212395e-04.:  55%|███████████████████████████████████████████████████▉                                              epoch 1 iter 63: train loss 3.25689. lr 5.212395e-04.:  56%|████████████████████████████████████████████████████▊                                             epoch 1 iter 64: train loss 3.32415. lr 5.188299e-04.:  56%|████████████████████████████████████████████████████▊                                             epoch 1 iter 64: train loss 3.32415. lr 5.188299e-04.:  57%|█████████████████████████████████████████████████████▌                           epoch 1 iter 65: train loss 3.36210. lr 5.163897e-04.:  57%|█████████████████████████████████████████████████████▌                                     epoch 1 iter 65: train loss 3.36210. lr 5.163897e-04.:  58%|██████████████████████████████████████████████████████▍                                           epoch 1 iter 66: train loss 3.35941. lr 5.139194e-04.:  58%|██████████████████████████████████████████████████████▍                                           epoch 1 iter 66: train loss 3.35941. lr 5.139194e-04.:  59%|███████████████████████████████████████████████████████▏                                          epoch 1 iter 67: train loss 3.32300. lr 5.114191e-04.:  59%|███████████████████████████████████████████████████████▏                                          epoch 1 iter 67: train loss 3.32300. lr 5.114191e-04.:  60%|████████████████████████████████████████████████████████                                          epoch 1 iter 68: train loss 3.23763. lr 5.088893e-04.:  60%|████████████████████████████████████████████████████████                                          epoch 1 iter 68: train loss 3.23763. lr 5.088893e-04.:  61%|████████████████████████████████████████████████████████▉                                         epoch 1 iter 69: train loss 3.39644. lr 5.063304e-04.:  61%|████████████████████████████████████████████████████████▉                                         epoch 1 iter 69: train loss 3.39644. lr 5.063304e-04.:  61%|█████████████████████████████████████████████████████████▋                                        epoch 1 iter 70: train loss 3.17761. lr 5.037427e-04.:  61%|█████████████████████████████████████████████████████████▋                                        epoch 1 iter 70: train loss 3.17761. lr 5.037427e-04.:  62%|██████████████████████████████████████████████████████████▌                                       epoch 1 iter 71: train loss 3.40761. lr 5.011265e-04.:  62%|██████████████████████████████████████████████████████████▌                                       epoch 1 iter 71: train loss 3.40761. lr 5.011265e-04.:  63%|███████████████████████████████████████████████████████████▎             epoch 1 iter 72: train loss 3.34361. lr 4.984822e-04.:  63%|███████████████████████████████████████████████████████████▎                       epoch 1 iter 72: train loss 3.34361. lr 4.984822e-04.:  64%|████████████████████████████████████████████████████████████▏                                epoch 1 iter 73: train loss 3.31184. lr 4.958102e-04.:  64%|████████████████████████████████████████████████████████████▏                                     epoch 1 iter 73: train loss 3.31184. lr 4.958102e-04.:  65%|█████████████████████████████████████████████████████████████                                     epoch 1 iter 74: train loss 3.29936. lr 4.931109e-04.:  65%|█████████████████████████████████████████████████████████████                                     epoch 1 iter 74: train loss 3.29936. lr 4.931109e-04.:  66%|█████████████████████████████████████████████████████████████▊                                    epoch 1 iter 75: train loss 3.06336. lr 4.903846e-04.:  66%|█████████████████████████████████████████████████████████████▊                                    epoch 1 iter 75: train loss 3.06336. lr 4.903846e-04.:  67%|██████████████████████████████████████████████████████████████▋                                   epoch 1 iter 76: train loss 3.32557. lr 4.876318e-04.:  67%|██████████████████████████████████████████████████████████████▋                                   epoch 1 iter 76: train loss 3.32557. lr 4.876318e-04.:  68%|███████████████████████████████████████████████████████████████▍                                  epoch 1 iter 77: train loss 3.20728. lr 4.848527e-04.:  68%|███████████████████████████████████████████████████████████████▍                                  epoch 1 iter 77: train loss 3.20728. lr 4.848527e-04.:  68%|████████████████████████████████████████████████████████████████▎                                 epoch 1 iter 78: train loss 3.19726. lr 4.820478e-04.:  68%|████████████████████████████████████████████████████████████████▎                                 epoch 1 iter 78: train loss 3.19726. lr 4.820478e-04.:  69%|█████████████████████████████████████████████████████████████████▏                                epoch 1 iter 79: train loss 3.32379. lr 4.792175e-04.:  69%|█████████████████████████████████████████████████████████████████▏         epoch 1 iter 79: train loss 3.32379. lr 4.792175e-04.:  70%|█████████████████████████████████████████████████████████████████▉                   epoch 1 iter 80: train loss 3.37819. lr 4.763622e-04.:  70%|█████████████████████████████████████████████████████████████████▉                             epoch 1 iter 80: train loss 3.37819. lr 4.763622e-04.:  71%|██████████████████████████████████████████████████████████████████▊                               epoch 1 iter 81: train loss 3.14178. lr 4.734822e-04.:  71%|██████████████████████████████████████████████████████████████████▊                               epoch 1 iter 81: train loss 3.14178. lr 4.734822e-04.:  72%|███████████████████████████████████████████████████████████████████▌                              epoch 1 iter 82: train loss 3.27431. lr 4.705781e-04.:  72%|███████████████████████████████████████████████████████████████████▌                              epoch 1 iter 82: train loss 3.27431. lr 4.705781e-04.:  73%|████████████████████████████████████████████████████████████████████▍                             epoch 1 iter 83: train loss 3.16843. lr 4.676501e-04.:  73%|████████████████████████████████████████████████████████████████████▍                             epoch 1 iter 83: train loss 3.16843. lr 4.676501e-04.:  74%|█████████████████████████████████████████████████████████████████████▎                            epoch 1 iter 84: train loss 3.25917. lr 4.646987e-04.:  74%|█████████████████████████████████████████████████████████████████████▎                            epoch 1 iter 84: train loss 3.25917. lr 4.646987e-04.:  75%|██████████████████████████████████████████████████████████████████████                            epoch 1 iter 85: train loss 3.17491. lr 4.617243e-04.:  75%|██████████████████████████████████████████████████████████████████████                            epoch 1 iter 85: train loss 3.17491. lr 4.617243e-04.:  75%|██████████████████████████████████████████████████████████████████████▉                           epoch 1 iter 86: train loss 3.24801. lr 4.587273e-04.:  75%|██████████████████████████████████████████████████████████████████████▉                           epoch 1 iter 86: train loss 3.24801. lr 4.587273e-04.:  76%|███████████████████████████████████████████████████████████████████████▋     epoch 1 iter 87: train loss 3.16179. lr 4.557082e-04.:  76%|███████████████████████████████████████████████████████████████████████▋               epoch 1 iter 87: train loss 3.16179. lr 4.557082e-04.:  77%|████████████████████████████████████████████████████████████████████████▌                        epoch 1 iter 88: train loss 3.04168. lr 4.526673e-04.:  77%|████████████████████████████████████████████████████████████████████████▌                         epoch 1 iter 88: train loss 3.04168. lr 4.526673e-04.:  78%|█████████████████████████████████████████████████████████████████████████                         epoch 1 iter 89: train loss 3.33297. lr 4.496051e-04.:  78%|█████████████████████████████████████████████████████████████████████████                         epoch 1 iter 89: train loss 3.33297. lr 4.496051e-04.:  79%|█████████████████████████████████████████████████████████████████████████                         epoch 1 iter 90: train loss 3.12021. lr 4.465220e-04.:  79%|██████████████████████████████████████████████████████████████████████████▏                       epoch 1 iter 90: train loss 3.12021. lr 4.465220e-04.:  80%|███████████████████████████████████████████████████████████████████████████                       epoch 1 iter 91: train loss 3.33258. lr 4.434184e-04.:  80%|███████████████████████████████████████████████████████████████████████████                       epoch 1 iter 91: train loss 3.33258. lr 4.434184e-04.:  81%|███████████████████████████████████████████████████████████████████████████▊                      epoch 1 iter 92: train loss 3.24330. lr 4.402949e-04.:  81%|███████████████████████████████████████████████████████████████████████████▊                      epoch 1 iter 92: train loss 3.24330. lr 4.402949e-04.:  82%|████████████████████████████████████████████████████████████████████████████▋                     epoch 1 iter 93: train loss 3.39295. lr 4.371517e-04.:  82%|████████████████████████████████████████████████████████████████████████████▋                     epoch 1 iter 93: train loss 3.39295. lr 4.371517e-04.:  82%|█████████████████████████████████████████████████████████████████████████████▌                    epoch 1 iter 94: train loss 3.09150. lr 4.339894e-04.:  82%|█████████████████████████████████████████████████████████████████████████████▌             epoch 1 iter 94: train loss 3.09150. lr 4.339894e-04.:  83%|██████████████████████████████████████████████████████████████████████████████▎                   epoch 1 iter 95: train loss 3.03029. lr 4.308084e-04.:  83%|██████████████████████████████████████████████████████████████████████████████▎                   epoch 1 iter 95: train loss 3.03029. lr 4.308084e-04.:  84%|███████████████████████████████████████████████████████████████████████████████▏                  epoch 1 iter 96: train loss 3.04656. lr 4.276091e-04.:  84%|███████████████████████████████████████████████████████████████████████████████▏                  epoch 1 iter 96: train loss 3.04656. lr 4.276091e-04.:  85%|███████████████████████████████████████████████████████████████████████████████▉                  epoch 1 iter 97: train loss 3.18074. lr 4.243920e-04.:  85%|███████████████████████████████████████████████████████████████████████████████▉                  epoch 1 iter 97: train loss 3.18074. lr 4.243920e-04.:  86%|████████████████████████████████████████████████████████████████████████████████▊                 epoch 1 iter 98: train loss 3.22283. lr 4.211575e-04.:  86%|████████████████████████████████████████████████████████████████████████████████▊                 epoch 1 iter 98: train loss 3.22283. lr 4.211575e-04.:  87%|█████████████████████████████████████████████████████████████████████████████████▋                epoch 1 iter 99: train loss 3.28953. lr 4.179061e-04.:  87%|█████████████████████████████████████████████████████████████████████████████████▋                epoch 1 iter 99: train loss 3.28953. lr 4.179061e-04.:  88%|█████████████████████████████████████████████████████████████████████████████████▌                epoch 1 iter 100: train loss 3.36845. lr 4.146383e-04.:  88%|████████████████████████████████████████████████████████████████████████████████▋                epoch 1 iter 100: train loss 3.36845. lr 4.146383e-04.:  89%|█████████████████████████████████████████████████████████████████████████████████▌               epoch 1 iter 101: train loss 3.20094. lr 4.113544e-04.:  89%|█████████████████████████████████████████████████████████████████████████████████▌               epoch 1 iter 101: train loss 3.20094. lr 4.113544e-04.:  89%|██████████████████████████████████████████████████████████████████████████████████▎         epoch 1 iter 102: train loss 3.15607. lr 4.080550e-04.:  89%|██████████████████████████████████████████████████████████████████████████████████▎              epoch 1 iter 102: train loss 3.15607. lr 4.080550e-04.:  90%|███████████████████████████████████████████████████████████████████████████████████              epoch 1 iter 103: train loss 2.96083. lr 4.047405e-04.:  90%|███████████████████████████████████████████████████████████████████████████████████              epoch 1 iter 103: train loss 2.96083. lr 4.047405e-04.:  91%|███████████████████████████████████████████████████████████████████████████████████▉             epoch 1 iter 104: train loss 2.99713. lr 4.014114e-04.:  91%|███████████████████████████████████████████████████████████████████████████████████▉             epoch 1 iter 104: train loss 2.99713. lr 4.014114e-04.:  92%|████████████████████████████████████████████████████████████████████████████████████             epoch 1 iter 105: train loss 3.14808. lr 3.980681e-04.:  92%|████████████████████████████████████████████████████████████████████████████████████             epoch 1 iter 105: train loss 3.14808. lr 3.980681e-04.:  93%|████████████████████████████████████████████████████████████████████████████████████             epoch 1 iter 106: train loss 2.85821. lr 3.947112e-04.:  93%|████████████████████████████████████████████████████████████████████████████████████             epoch 1 iter 106: train loss 2.85821. lr 3.947112e-04.:  94%|████████████████████████████████████████████████████████████████████████████████████             epoch 1 iter 107: train loss 3.17177. lr 3.913410e-04.:  94%|██████████████████████████████████████████████████████████████████████████████████████▎     | 107epoch 1 iter 107: train loss 3.17177. lr 3.913410e-04.:  95%|███████████████████████████████████████████████████████████████████████████████████████▏    | 108epoch 1 iter 108: train loss 3.03921. lr 3.879580e-04.:  95%|███████████████████████████████████████████████████████████████████████████████████████▏    | 108epoch 1 iter 108: train loss 3.03921. lr 3.879580e-04.:  96%|███████████████████████████████████████████████████████████████████████████████████████▉    | 109epoch 1 iter 109: train loss 3.05947. lr 3.845628e-04.:  96%|███████████████████████████████████████████████████████████████████████████████████████▉    | 109epoch 1 iter 109: train loss 3.05947. lr 3.845628e-04.:  96%|████████████████████████████████████████████████████████████████████████████████████████▊   | 110epoch 1 iter 110: train loss 2.93004. lr 3.811558e-04.:  96%|████████████████████████████████████████████████████████████████████████████████████████▊   | 110epoch 1 iter 110: train loss 2.93004. lr 3.811558e-04.:  97%|█████████████████████████████████████████████████████████████████████████████████████████▌  | 111epoch 1 iter 111: train loss 2.98517. lr 3.777374e-04.:  97%|█████████████████████████████████████████████████████████████████████████████████████████▌  | 111epoch 1 iter 111: train loss 2.98517. lr 3.777374e-04.:  98%|██████████████████████████████████████████████████████████████████████████████████████████▍ | 112epoch 1 iter 112: train loss 3.02494. lr 3.743082e-04.:  98%|██████████████████████████████████████████████████████████████████████████████████████████▍ | 112epoch 1 iter 112: train loss 3.02494. lr 3.743082e-04.:  99%|███████████████████████████████████████████████████████████████████████████████████████████▏| 113epoch 1 iter 113: train loss 3.42489. lr 3.709762e-04.:  99%|███████████████████████████████████████████████████████████████████████████████████████████▏| 113epoch 1 iter 113: train loss 3.42489. lr 3.709762e-04.: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 114epoch 1 iter 113: train loss 3.42489. lr 3.709762e-04.: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [08:26<00:00,  4.45s/it]
epoch 2 iter 0: train loss 3.10828. lr 3.675270e-04.:   0%|                                                                           | 0/114 [00:06epoch 2 iter 0: train loss 3.10828. lr 3.675270e-04.:   1%|▌                                                                  | 1/114 [00:06<12:16, epoch 2 iter 1: train loss 3.01201. lr 3.640684e-04.:   1%|▌                                                                  | 1/114 [00:11<12:16, epoch 2 iter 1: train loss 3.01201. lr 3.640684e-04.:   2%|█▏                                                                 | 2/114 [00:11<10:07, epoch 2 iter 2: train loss 3.14122. lr 3.606008e-04.:   2%|█▏                                                                 | 2/114 [00:15<10:07, epoch 2 iter 2: train loss 3.14122. lr 3.606008e-04.:   3%|█▊                                                                 | 3/114 [00:15<09:12, epoch 2 iter 3: train loss 3.08320. lr 3.571248e-04.:   3%|█▊                                                                 | 3/114 [00:20<09:12, epoch 2 iter 3: train loss 3.08320. lr 3.571248e-04.:   4%|██▎                                                                | 4/114 [00:20<08:46, epoch 2 iter 4: train loss 3.04508. lr 3.536408e-04.:   4%|██▎                                                                | 4/114 [00:24<08:46, epoch 2 iter 4: train loss 3.04508. lr 3.536408e-04.:   4%|██▉                                                                | 5/114 [00:24<08:26, epoch 2 iter 5: train loss 3.03954. lr 3.501493e-04.:   4%|██▉                                                                | 5/114 [00:29<08:26, epoch 2 iter 5: train loss 3.03954. lr 3.501493e-04.:   5%|███▌                                                               | 6/114 [00:29<08:20, epoch 2 iter 6: train loss 3.08850. lr 3.466508e-04.:   5%|███▌                                                               | 6/114 [00:33<08:20, epoch 2 iter 6: train loss 3.08850. lr 3.466508e-04.:   6%|████                                                               | 7/114 [00:33<08:12, epoch 2 iter 7: train loss 3.02259. lr 3.431458e-04.:   6%|████                                                               | 7/114 [00:38<08:12, epoch 2 iter 7: train loss 3.02259. lr 3.431458e-04.:   7%|████▋                                                              | 8/114 [00:38<07:59, epoch 2 iter 8: train loss 3.10789. lr 3.396347e-04.:   7%|████▋                                                              | 8/114 [00:42<07:59, epoch 2 iter 8: train loss 3.10789. lr 3.396347e-04.:   8%|█████▎                                                             | 9/114 [00:42<07:50, epoch 2 iter 9: train loss 2.80169. lr 3.361182e-04.:   8%|█████▎                                                             | 9/114 [00:46<07:50, epoch 2 iter 9: train loss 2.80169. lr 3.361182e-04.:   9%|█████▊                                                            | 10/114 [00:46<07:42, epoch 2 iter 10: train loss 3.23121. lr 3.325966e-04.:   9%|█████▋                                                           | 10/114 [00:51<07:42, epoch 2 iter 10: train loss 3.23121. lr 3.325966e-04.:  10%|██████▎                                                          | 11/114 [00:51<07:32, epoch 2 iter 11: train loss 2.89646. lr 3.290704e-04.:  10%|██████▎                                                          | 11/114 [00:55<07:32, epoch 2 iter 11: train loss 2.89646. lr 3.290704e-04.:  11%|██████▊                                                          | 12/114 [00:55<07:29, epoch 2 iter 12: train loss 3.00706. lr 3.255402e-04.:  11%|██████▊                                                          | 12/114 [00:59<07:29, epoch 2 iter 12: train loss 3.00706. lr 3.255402e-04.:  11%|███████▍                                                         | 13/114 [00:59<07:26, epoch 2 iter 13: train loss 2.92299. lr 3.220064e-04.:  11%|███████▍                                                         | 13/114 [01:04<07:26, epoch 2 iter 13: train loss 2.92299. lr 3.220064e-04.:  12%|███████▉                                                         | 14/114 [01:04<07:26, epoch 2 iter 73: train loss 2.93497. lr 1.219088e-04.:  64%|█████████████████████████████████████████▌                       | 73/114 [05:29<02:55, epoch 2 iter 73: train loss 2.93497. lr 1.219088e-04.:  65%|██████████████████████████████████████████▏                      | 74/114 [05:29<02:50, epoch 2 iter 74: train loss 3.03422. lr 1.190686e-04.:  65%|██████████████████████████████████████████▏                      | 74/114 [05:34<02:50, epoch 2 iter 74: train loss 3.03422. lr 1.190686e-04.:  66%|██████████████████████████████████████████▊                      | 75/114 [05:34<02:48, epoch 2 iter 75: train loss 2.83256. lr 1.162536e-04.:  66%|██████████████████████████████████████████▊                      | 75/114 [05:38<02:48, epoch 2 iter 75: train loss 2.83256. lr 1.162536e-04.:  67%|███████████████████████████████████████████▎                     | 76/114 [05:38<02:46, epoch 2 iter 76: train loss 2.70668. lr 1.134643e-04.:  67%|███████████████████████████████████████████▎                     | 76/114 [05:43<02:46, epoch 2 iter 76: train loss 2.70668. lr 1.134643e-04.:  68%|███████████████████████████████████████████▉                     | 77/114 [05:43<02:43, epoch 2 iter 77: train loss 2.88239. lr 1.107010e-04.:  68%|███████████████████████████████████████████▉                     | 77/114 [05:47<02:43, epoch 2 iter 77: train loss 2.88239. lr 1.107010e-04.:  68%|████████████████████████████████████████████▍                    | 78/114 [05:47<02:38, epoch 2 iter 78: train loss 2.71435. lr 1.079641e-04.:  68%|████████████████████████████████████████████▍                    | 78/114 [05:52<02:38, epoch 2 iter 78: train loss 2.71435. lr 1.079641e-04.:  69%|█████████████████████████████████████████████                    | 79/114 [05:52<02:34, epoch 2 iter 79: train loss 2.81776. lr 1.052541e-04.:  69%|█████████████████████████████████████████████                    | 79/114 [05:56<02:34, epoch 2 iter 79: train loss 2.81776. lr 1.052541e-04.:  70%|█████████████████████████████████████████████▌                   | 80/114 [05:56<02:30, epoch 2 iter 80: train loss 2.92842. lr 1.025713e-04.:  70%|█████████████████████████████████████████████▌                   | 80/114 [06:00<02:30, epoch 2 iter 80: train loss 2.92842. lr 1.025713e-04.:  71%|██████████████████████████████████████████████▏                  | 81/114 [06:00<02:22, epoch 2 iter 81: train loss 2.96567. lr 9.991600e-05.:  71%|██████████████████████████████████████████████▏                  | 81/114 [06:05<02:22, epoch 2 iter 81: train loss 2.96567. lr 9.991600e-05.:  72%|██████████████████████████████████████████████▊                  | 82/114 [06:05<02:20, epoch 2 iter 82: train loss 2.92416. lr 9.728866e-05.:  72%|██████████████████████████████████████████████▊                  | 82/114 [06:09<02:20, epoch 2 iter 82: train loss 2.92416. lr 9.728866e-05.:  73%|███████████████████████████████████████████████▎                 | 83/114 [06:09<02:18, epoch 2 iter 83: train loss 2.77199. lr 9.468963e-05.:  73%|███████████████████████████████████████████████▎                 | 83/114 [06:14<02:18, epoch 2 iter 83: train loss 2.77199. lr 9.468963e-05.:  74%|███████████████████████████████████████████████▉                 | 84/114 [06:14<02:15, epoch 2 iter 84: train loss 2.91350. lr 9.211926e-05.:  74%|███████████████████████████████████████████████▉                 | 84/114 [06:18<02:15, epoch 2 iter 84: train loss 2.91350. lr 9.211926e-05.:  75%|████████████████████████████████████████████████▍                | 85/114 [06:18<02:10, epoch 2 iter 85: train loss 2.86571. lr 8.957792e-05.:  75%|████████████████████████████████████████████████▍                | 85/114 [06:23<02:10, epoch 2 iter 85: train loss 2.86571. lr 8.957792e-05.:  75%|█████████████████████████████████████████████████                | 86/114 [06:23<02:07, epoch 2 iter 86: train loss 2.97729. lr 8.706596e-05.:  75%|█████████████████████████████████████████████████                | 86/114 [06:28<02:07, epoch 2 iter 86: train loss 2.97729. lr 8.706596e-05.:  76%|█████████████████████████████████████████████████▌               | 87/114 [06:28<02:02, epoch 2 iter 87: train loss 2.71229. lr 8.458373e-05.:  76%|█████████████████████████████████████████████████▌               | 87/114 [06:32<02:02, epoch 2 iter 87: train loss 2.71229. lr 8.458373e-05.:  77%|██████████████████████████████████████████████████▏              | 88/114 [06:32<01:57, epoch 2 iter 88: train loss 2.80151. lr 8.213158e-05.:  77%|██████████████████████████████████████████████████▏              | 88/114 [06:36<01:57, epoch 2 iter 88: train loss 2.80151. lr 8.213158e-05.:  78%|██████████████████████████████████████████████████▋              | 89/114 [06:36<01:52, epoch 2 iter 89: train loss 2.86030. lr 7.970986e-05.:  78%|██████████████████████████████████████████████████▋              | 89/114 [06:41<01:52, epoch 2 iter 89: train loss 2.86030. lr 7.970986e-05.:  79%|███████████████████████████████████████████████████▎             | 90/114 [06:41<01:46, epoch 2 iter 90: train loss 2.65015. lr 7.731889e-05.:  79%|███████████████████████████████████████████████████▎             | 90/114 [06:45<01:46, epoch 2 iter 90: train loss 2.65015. lr 7.731889e-05.:  80%|███████████████████████████████████████████████████▉             | 91/114 [06:45<01:41, epoch 2 iter 91: train loss 2.91412. lr 7.495901e-05.:  80%|███████████████████████████████████████████████████▉             | 91/114 [06:49<01:41, epoch 2 iter 91: train loss 2.91412. lr 7.495901e-05.:  81%|████████████████████████████████████████████████████▍            | 92/114 [06:49<01:33, epoch 2 iter 92: train loss 2.96468. lr 7.263056e-05.:  81%|████████████████████████████████████████████████████▍            | 92/114 [06:54<01:33, epoch 2 iter 92: train loss 2.96468. lr 7.263056e-05.:  82%|█████████████████████████████████████████████████████            | 93/114 [06:54<01:31, epoch 2 iter 93: train loss 2.65180. lr 7.033385e-05.:  82%|█████████████████████████████████████████████████████            | 93/114 [06:58<01:31, epoch 2 iter 93: train loss 2.65180. lr 7.033385e-05.:  82%|█████████████████████████████████████████████████████▌           | 94/114 [06:58<01:28, epoch 2 iter 94: train loss 2.74577. lr 6.806921e-05.:  82%|█████████████████████████████████████████████████████▌           | 94/114 [07:03<01:28, epoch 2 iter 94: train loss 2.74577. lr 6.806921e-05.:  83%|██████████████████████████████████████████████████████▏          | 95/114 [07:03<01:25, epoch 2 iter 95: train loss 2.86722. lr 6.583695e-05.:  83%|██████████████████████████████████████████████████████▏          | 95/114 [07:07<01:25, epoch 2 iter 95: train loss 2.86722. lr 6.583695e-05.:  84%|██████████████████████████████████████████████████████▋          | 96/114 [07:07<01:20, epoch 2 iter 96: train loss 3.07135. lr 6.363739e-05.:  84%|██████████████████████████████████████████████████████▋          | 96/114 [07:11<01:20, epoch 2 iter 96: train loss 3.07135. lr 6.363739e-05.:  85%|███████████████████████████████████████████████████████▎         | 97/114 [07:11<01:14, epoch 2 iter 97: train loss 3.12225. lr 6.147084e-05.:  85%|███████████████████████████████████████████████████████▎         | 97/114 [07:16<01:14, epoch 2 iter 97: train loss 3.12225. lr 6.147084e-05.:  86%|███████████████████████████████████████████████████████▉         | 98/114 [07:16<01:10, epoch 2 iter 98: train loss 2.96945. lr 6.000000e-05.:  86%|███████████████████████████████████████████████████████▉         | 98/114 [07:20<01:10, epoch 2 iter 98: train loss 2.96945. lr 6.000000e-05.:  87%|████████████████████████████████████████████████████████▍        | 99/114 [07:20<01:04, epoch 2 iter 99: train loss 2.74381. lr 6.000000e-05.:  87%|████████████████████████████████████████████████████████▍        | 99/114 [07:24<01:04, epoch 2 iter 99: train loss 2.74381. lr 6.000000e-05.:  88%|████████████████████████████████████████████████████████▏       | 100/114 [07:24<00:59, epoch 2 iter 100: train loss 2.59047. lr 6.000000e-05.:  88%|███████████████████████████████████████████████████████▎       | 100/114 [07:29<00:59, epoch 2 iter 100: train loss 2.59047. lr 6.000000e-05.:  89%|███████████████████████████████████████████████████████▊       | 101/114 [07:29<00:55, epoch 2 iter 101: train loss 2.80223. lr 6.000000e-05.:  89%|███████████████████████████████████████████████████████▊       | 101/114 [07:33<00:55, epoch 2 iter 101: train loss 2.80223. lr 6.000000e-05.:  89%|████████████████████████████████████████████████████████▎      | 102/114 [07:33<00:52, epoch 2 iter 102: train loss 3.17486. lr 6.000000e-05.:  89%|████████████████████████████████████████████████████████▎      | 102/114 [07:37<00:52, epoch 2 iter 102: train loss 3.17486. lr 6.000000e-05.:  90%|████████████████████████████████████████████████████████▉      | 103/114 [07:37<00:48, epoch 2 iter 103: train loss 2.59353. lr 6.000000e-05.:  90%|████████████████████████████████████████████████████████▉      | 103/114 [07:42<00:48, epoch 2 iter 103: train loss 2.59353. lr 6.000000e-05.:  91%|█████████████████████████████████████████████████████████▍     | 104/114 [07:42<00:44, epoch 2 iter 104: train loss 2.74771. lr 6.000000e-05.:  91%|█████████████████████████████████████████████████████████▍     | 104/114 [07:46<00:44, epoch 2 iter 104: train loss 2.74771. lr 6.000000e-05.:  92%|██████████████████████████████████████████████████████████     | 105/114 [07:46<00:39, epoch 2 iter 105: train loss 2.74570. lr 6.000000e-05.:  92%|██████████████████████████████████████████████████████████     | 105/114 [07:51<00:39, epoch 2 iter 105: train loss 2.74570. lr 6.000000e-05.:  93%|██████████████████████████████████████████████████████████▌    | 106/114 [07:51<00:35, epoch 2 iter 106: train loss 2.85760. lr 6.000000e-05.:  93%|██████████████████████████████████████████████████████████▌    | 106/114 [07:55<00:35, epoch 2 iter 106: train loss 2.85760. lr 6.000000e-05.:  94%|███████████████████████████████████████████████████████████▏   | 107/114 [07:55<00:30, epoch 2 iter 107: train loss 2.64313. lr 6.000000e-05.:  94%|███████████████████████████████████████████████████████████▏   | 107/114 [07:59<00:30, epoch 2 iter 107: train loss 2.64313. lr 6.000000e-05.:  95%|███████████████████████████████████████████████████████████▋   | 108/114 [07:59<00:26, epoch 2 iter 108: train loss 2.84827. lr 6.000000e-05.:  95%|███████████████████████████████████████████████████████████▋   | 108/114 [08:04<00:26, epoch 2 iter 108: train loss 2.84827. lr 6.000000e-05.:  96%|████████████████████████████████████████████████████████████▏  | 109/114 [08:04<00:22, epoch 2 iter 109: train loss 2.90762. lr 6.000000e-05.:  96%|████████████████████████████████████████████████████████████▏  | 109/114 [08:09<00:22, epoch 2 iter 109: train loss 2.90762. lr 6.000000e-05.:  96%|████████████████████████████████████████████████████████████▊  | 110/114 [08:09<00:17, epoch 2 iter 110: train loss 2.69450. lr 6.000000e-05.:  96%|████████████████████████████████████████████████████████████▊  | 110/114 [08:13<00:17, epoch 2 iter 110: train loss 2.69450. lr 6.000000e-05.:  97%|█████████████████████████████████████████████████████████████▎ | 111/114 [08:13<00:13, epoch 2 iter 111: train loss 2.74031. lr 6.000000e-05.:  97%|█████████████████████████████████████████████████████████████▎ | 111/114 [08:18<00:13, epoch 2 iter 111: train loss 2.74031. lr 6.000000e-05.:  98%|█████████████████████████████████████████████████████████████▉ | 112/114 [08:18<00:08, epoch 2 iter 112: train loss 2.75179. lr 6.000000e-05.:  98%|█████████████████████████████████████████████████████████████▉ | 112/114 [08:22<00:08, epoch 2 iter 112: train loss 2.75179. lr 6.000000e-05.:  99%|██████████████████████████████████████████████████████████████▍| 113/114 [08:22<00:04, epoch 2 iter 113: train loss 3.10088. lr 6.000000e-05.:  99%|██████████████████████████████████████████████████████████████▍| 113/114 [08:27<00:04, epoch 2 iter 113: train loss 3.10088. lr 6.000000e-05.: 100%|███████████████████████████████████████████████████████████████| 114/114 [08:27<00:00, epoch 2 iter 113: train loss 3.10088. lr 6.000000e-05.: 100%|███████████████████████████████████████████████████████████████| 114/114 [08:27<00:00,  4.45s/it]
epoch 3 iter 113: train loss 3.11398. lr 1.097304e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:29<00:00,  4.47s/it]
epoch 4 iter 105: train loss 2.85653. lr 4.607609e-04.:  93%|█████████████████████████████████████████████████▎   | 106/114 [07:56<00:36,  4.52s/it]testing returns...
test_res tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,
        18, 18,  3,  3, 18, 18, 18, 11, 18, 18, 18, 18, 18, 18, 11, 11, 11, 26,
        18, 26, 18, 26, 26, 26, 26, 49, 49, 49, 49, 49, 56, 56, 56, 56, 56, 49,
        56, 56, 56, 56, 56, 48, 56, 56, 56, 56, 56, 56, 56, 56, 48, 56, 56, 49,
        48, 56, 58, 48, 48, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58,
        58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58,
        58, 58, 51,  5, 13,  5, 13,  5, 13, 13,  5, 13, 20, 20, 13, 20, 20, 13,
        13, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 52, 52, 60,
        60, 60, 52, 60, 52, 52, 52, 52, 52, 52, 60, 52, 52, 52, 52, 52, 52, 60,
        52, 52, 52, 52, 60, 52, 52, 52, 52, 52, 52, 52, 22, 22, 22, 22, 22, 22,
        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,
        22, 22, 22, 22, 22, 22, 22, 22, 55, 55, 55, 55, 55, 39, 39, 39, 39, 39,
        39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 55, 39,
        39, 39, 39, 39])
test_res shape torch.Size([256])
test_targets tensor([24,  1, 16, 17,  9, 25, 16,  1, 17,  9, 17, 24, 17, 17, 17,  9, 16, 24,
        25, 24,  1, 24, 16, 16,  9, 25, 24, 24, 24, 24, 25, 17, 11, 19, 26, 27,
        19, 26, 19, 18, 18, 26, 19,  3, 18, 19, 26, 11,  3, 18, 11, 27, 11, 19,
        27,  3, 27, 19, 26, 18, 26, 18, 18, 19, 49, 56, 56, 49, 56, 41, 56, 49,
        56, 41, 33, 56, 49, 57, 33, 48, 48, 33, 56, 48, 56, 56, 56, 57, 56, 48,
        49, 33, 48, 33, 48, 41, 59, 43, 58, 58, 58, 51, 58, 35, 51, 51, 43, 50,
        58, 58, 43, 35, 59, 50, 59, 59, 58, 51, 50, 58, 51, 43, 58, 51, 50, 50,
        51, 50, 20,  5,  5, 21, 21, 29, 13, 28, 21, 13,  5, 28, 29, 28, 29, 28,
        29, 28, 28, 13, 28, 13, 20, 29, 28, 20, 20, 21, 20, 21, 13,  5, 60, 52,
        61, 52, 61, 53, 45, 52, 45, 45, 45, 45, 60, 60, 60, 37, 60, 53, 37, 45,
        53, 37, 60, 61, 60, 45, 60, 37, 60, 53, 37, 45, 22, 15, 31, 15, 31, 23,
        15, 22, 30, 22, 23, 22,  7, 30, 22, 30, 15, 31, 30, 31,  7,  7, 23,  7,
        22, 15, 15, 22,  7, 30, 15, 31, 63, 55, 54, 62, 47, 47, 63, 54, 63, 54,
        63, 63, 54, 55, 63, 47, 63, 63, 55, 39, 47, 47, 54, 62, 63, 39, 47, 39,
        62, 54, 47, 54])
test_targets shape torch.Size([256])
epoch 4 iter 113: train loss 2.83823. lr 4.838640e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:33<00:00,  4.51s/it]
epoch 5 iter 113: train loss 2.61365. lr 5.719827e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:45<00:00,  4.61s/it]
epoch 6 iter 113: train loss 2.67043. lr 2.370109e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:32<00:00,  4.50s/it]
epoch 7 iter 113: train loss 2.64953. lr 6.000000e-05.: 100%|█████████████████████████████████████████████████████| 114/114 [08:38<00:00,  4.54s/it]
epoch 8 iter 113: train loss 2.25841. lr 2.296731e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.56s/it]
epoch 9 iter 113: train loss 2.79724. lr 5.687216e-04.: 100%|█████████████████████████████████████████████████████| 114/114 [08:38<00:00,  4.55s/it]
epoch 10 iter 113: train loss 2.55979. lr 4.897525e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.56s/it]
epoch 11 iter 113: train loss 2.46597. lr 1.156085e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 12 iter 113: train loss 2.24166. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:37<00:00,  4.54s/it]
epoch 13 iter 113: train loss 2.26282. lr 3.636422e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.56s/it]
epoch 14 iter 113: train loss 2.71164. lr 5.999840e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:45<00:00,  4.61s/it]
epoch 15 iter 113: train loss 2.44956. lr 3.696772e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:46<00:00,  4.62s/it]
epoch 16 iter 113: train loss 2.35641. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:43<00:00,  4.60s/it]
epoch 17 iter 113: train loss 2.08936. lr 1.107655e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.58s/it]
epoch 18 iter 113: train loss 2.01526. lr 4.849181e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:33<00:00,  4.50s/it]
epoch 19 iter 113: train loss 2.07599. lr 5.714161e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:41<00:00,  4.58s/it]
epoch 20 iter 113: train loss 1.81857. lr 2.357050e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:46<00:00,  4.62s/it]
epoch 21 iter 113: train loss 1.80848. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.58s/it]
epoch 22 iter 113: train loss 1.71069. lr 2.309729e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:34<00:00,  4.51s/it]
epoch 23 iter 113: train loss 1.93374. lr 5.693131e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.59s/it]
epoch 24 iter 113: train loss 1.71080. lr 4.887156e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.56s/it]
epoch 25 iter 113: train loss 1.46262. lr 1.145562e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:37<00:00,  4.54s/it]
epoch 26 iter 113: train loss 1.41248. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:38<00:00,  4.55s/it]
epoch 27 iter 113: train loss 1.42231. lr 3.649474e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.55s/it]
epoch 28 iter 34: train loss 1.51779. lr 4.771789e-04.:  31%|████████████████▌                                     | 35/114 [02:40<05:55,  4.50s/it]testing returns...
test_res tensor([24,  1,  1,  9,  9, 16, 25, 16, 25, 16, 16, 25, 25, 25, 25, 17, 25, 17,
        17, 17, 25, 25, 17, 25, 17, 25, 25, 25, 25, 25, 25, 25, 25, 27, 11, 11,
         3, 27, 11, 27, 27, 27, 27,  3, 19,  3,  3,  3,  3,  3,  3,  3, 26, 11,
         3,  3, 11, 11, 11,  3,  3, 11, 11,  3,  3, 48, 48, 48, 48, 48, 48, 48,
        57, 57, 57, 56, 56, 33, 56, 56, 41, 41, 41, 41, 56, 48, 56, 48, 48, 33,
        49, 56, 41, 49, 48, 41, 48, 43, 58, 35, 35, 58, 35, 59, 35, 43, 43, 35,
        35, 58, 43, 58, 43, 43, 43, 43, 59, 59, 50, 51, 51, 50, 59, 59, 50, 59,
        50, 59,  5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 13, 28, 28, 28,
        28, 28, 28, 13, 21, 28, 13, 21, 20, 28, 29, 20, 20, 13, 13, 20, 60, 60,
        61, 61, 52, 61, 60, 52, 61, 52, 52, 53, 52, 53, 52, 52, 52, 52, 60, 53,
        60, 60, 52, 52, 53, 53, 60, 53, 53, 61, 53, 22, 22, 22, 22, 15, 15, 31,
        30,  7, 15,  7, 22, 22, 30, 30, 30, 22,  7, 22,  7, 22, 22, 23, 22, 22,
        22, 22, 23, 22, 23, 22, 31,  7, 54, 54, 62, 62, 54, 62, 62, 62, 62, 62,
        54, 47, 62, 54, 63, 63, 47, 63, 62, 63, 62, 62, 62, 63, 62, 62, 62, 62,
        62, 62, 47, 47])
test_res shape torch.Size([256])
test_targets tensor([17,  9,  1, 24,  9, 24, 25, 16, 25,  1, 24, 16, 17,  1, 25, 25, 25,  9,
        17, 25, 24, 25,  1,  9, 16, 25,  1, 25, 17, 25, 25,  1,  3, 27, 18, 11,
        11, 27, 27, 11,  3, 11, 27, 27, 19,  3, 19, 26, 19, 26,  3,  3, 26, 26,
        11,  3, 11, 19, 27, 26, 19,  3, 11, 11, 41, 48, 48, 48, 48, 57, 49, 49,
        48, 57, 48, 57, 33, 41, 56, 56, 57, 41, 33, 41, 33, 56, 48, 33, 49, 41,
        49, 48, 56, 41, 49, 56, 59, 43, 58, 35, 58, 59, 35, 58, 35, 35, 59, 59,
        35, 59, 50, 58, 35, 43, 58, 51, 43, 43, 43, 51, 35, 35, 35, 59, 58, 50,
        50, 59, 13, 13,  5, 29, 29, 29, 29, 20, 28, 21, 29, 21, 21, 28, 21, 28,
        28, 20, 21,  5, 21, 13, 29, 29, 28, 20,  5, 28, 20, 13, 29, 13, 60, 52,
        61, 60, 60, 45, 60, 52, 61, 53, 37, 53, 45, 37, 60, 45, 61, 61, 53, 52,
        53, 52, 52, 52, 52, 61, 53, 60, 45, 60, 37, 60, 30, 22,  7, 22, 22,  7,
        30, 30, 31, 23,  7,  7, 15,  7, 31, 22, 15, 30,  7,  7, 23, 23, 22, 22,
        23, 15, 30, 30, 22, 23, 22, 31, 62, 55, 39, 55, 54, 54, 39, 47, 39, 62,
        55, 63, 63, 54, 55, 62, 62, 62, 62, 54, 39, 54, 62, 55, 62, 47, 63, 63,
        47, 62, 62, 47])
test_targets shape torch.Size([256])
epoch 28 iter 113: train loss 1.25800. lr 5.999948e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.56s/it]
epoch 29 iter 113: train loss 1.34346. lr 3.683767e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:41<00:00,  4.57s/it]
epoch 30 iter 22: train loss 1.17808. lr 2.874526e-04.:  20%|██████████▉                                           | 23/114 [01:48<07:05,  4.68s/it]testing returns...
test_res tensor([17, 24, 17, 17, 16, 16, 24, 24, 17, 26, 26, 26,  3, 11, 11, 11, 33, 57,
        57, 57, 56, 57, 41, 33, 51, 59, 35, 43, 43, 43, 58, 58,  5, 28, 28, 28,
        28,  5,  5,  5, 52, 61, 61, 45, 45, 45, 45, 52, 31, 23, 23, 23, 23, 22,
        23, 23, 62, 47, 47, 39, 39, 39, 39, 54, 54, 16, 16, 16, 16, 24, 24, 24,
        26, 26, 11,  3,  3, 11, 11, 27, 57, 48, 48, 57, 57, 57, 57, 41, 59, 59,
        59, 58, 58, 43, 43, 43, 21, 21, 13, 28, 13, 21, 13,  5, 60, 60, 60, 37,
        37, 37, 37, 45, 31, 15, 30, 30, 30, 30, 30, 30, 63, 47, 47, 47, 47, 47,
        62, 62, 62, 24, 24, 24, 24,  9, 24, 24, 11, 19, 27, 27, 27,  3,  3, 27,
        49, 49, 41, 41, 41, 33, 57, 41, 59, 58, 58, 35, 35, 35, 58, 51, 20, 20,
        20,  5,  5, 20,  5,  5, 60, 60, 60, 37, 37, 37, 37, 61, 22, 30, 30, 30,
        30, 30, 30, 30, 55, 63, 63, 63, 63, 63, 47, 39, 25, 17, 17,  9, 25, 25,
        24, 24, 11, 11, 11,  3, 18,  3, 27, 27, 48, 49, 41, 41, 41, 41, 41, 41,
        43, 43, 51, 51, 43, 59, 35, 35, 29, 13,  5, 29, 20, 28, 20, 29, 60, 60,
        60, 60, 61, 52, 60, 61, 22, 22, 15, 15, 22, 30, 22, 22, 63, 63, 54, 47,
        47, 47, 47, 55])
test_res shape torch.Size([256])
test_targets tensor([25,  9,  1, 25, 17, 16, 17, 24, 18, 26, 11, 26,  3, 11,  3, 11, 33, 48,
        57, 57, 56, 57, 41, 33, 51, 43, 51, 59, 43, 43, 58, 59,  5, 28, 28, 28,
        29, 20, 28,  5, 60, 53, 45, 60, 52, 61, 45, 45, 22, 23, 23, 31, 23, 22,
        30, 22, 62, 39, 47, 55, 39, 63, 39, 54, 16, 24, 17, 16, 16, 25, 24, 24,
        26, 27, 11,  3,  3,  3, 11, 27, 57, 56, 48, 48, 57, 49, 56, 41, 59, 35,
        59, 58, 51, 43, 43, 51, 20, 21, 13, 21, 13, 13,  5, 13, 52, 60, 37, 60,
        37, 61, 61, 45, 31, 15, 30, 15,  7,  7, 30, 31, 63, 47, 47, 39, 47, 47,
        55, 62, 25, 16,  1, 25, 17, 24,  9, 24, 11, 19, 27, 19, 27, 11,  3, 27,
        49, 33, 48, 41, 49, 41, 33, 56, 35, 58, 50, 35, 51, 59, 58, 43, 13, 20,
        28, 20, 29, 20, 29,  5, 52, 53, 60, 37, 60, 37, 53, 61, 30,  7, 31, 30,
        31,  7, 22, 30, 55, 63, 63, 54, 47, 62, 54, 55, 25, 17, 24,  9, 25, 25,
        24, 25, 11, 11, 26,  3, 18,  3,  3, 27, 48, 33, 49, 41, 41, 33, 57, 41,
        58, 43, 51, 58, 51, 50, 59, 35,  5,  5, 13, 29, 29, 28,  5, 20, 60, 60,
        37, 53, 60, 52, 37, 60, 22, 22, 15, 15,  7, 23, 22, 31, 63, 47, 47, 62,
        54, 39, 47, 62])
test_targets shape torch.Size([256])
epoch 30 iter 113: train loss 1.11287. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:47<00:00,  4.63s/it]
epoch 31 iter 113: train loss 1.05408. lr 1.118042e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:46<00:00,  4.62s/it]
epoch 32 iter 113: train loss 1.24361. lr 4.859685e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 33 iter 113: train loss 1.21221. lr 5.708442e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:45<00:00,  4.61s/it]
epoch 34 iter 113: train loss 1.09590. lr 2.344004e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 35 iter 113: train loss 0.87553. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:46<00:00,  4.62s/it]
epoch 36 iter 113: train loss 0.83381. lr 2.322740e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:38<00:00,  4.55s/it]
epoch 37 iter 113: train loss 1.14993. lr 5.698991e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:44<00:00,  4.60s/it]
epoch 38 iter 113: train loss 0.94471. lr 4.876750e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:49<00:00,  4.64s/it]
epoch 39 iter 113: train loss 0.71007. lr 1.135077e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.58s/it]
epoch 40 iter 113: train loss 0.70100. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:45<00:00,  4.61s/it]
epoch 41 iter 113: train loss 0.75251. lr 3.662514e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:36<00:00,  4.53s/it]
epoch 42 iter 113: train loss 0.92802. lr 5.999997e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.56s/it]
epoch 43 iter 113: train loss 0.69835. lr 3.670749e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 44 iter 113: train loss 0.66363. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.59s/it]
epoch 45 iter 60: train loss 0.62408. lr 6.000000e-05.:  54%|████████████████████████████▉                         | 61/114 [04:36<03:59,  4.52s/it]testing returns...
test_res tensor([24, 25,  1, 24,  9,  9, 25, 16, 25,  1, 16, 16, 17,  1, 25, 25, 25,  9,
        17, 25, 25, 25,  1,  9, 16, 25,  1, 25, 25, 25, 25,  1,  3, 27, 18, 11,
        11, 27, 11, 11, 27, 27, 27, 27, 19,  3, 19, 26, 19, 26,  3,  3, 26, 26,
        11, 11, 11, 19,  3,  3, 19, 19, 11, 11, 41, 48, 48, 48, 57, 57, 57, 49,
        48, 57, 48, 57, 57, 41, 56, 56, 41, 41, 33, 33, 41, 41, 41, 33, 49, 33,
        49, 56, 56, 56, 56, 56, 59, 43, 58, 35, 58, 59, 35, 58, 35, 59, 58, 59,
        43, 58, 58, 58, 43, 43, 58, 51, 58, 43, 43, 51, 35, 58, 59, 59, 59, 50,
        50, 59, 13, 13, 29, 29, 29, 29, 20, 21, 28, 29, 29, 29, 29, 28, 28, 28,
        28, 28, 21, 29, 20, 29, 20, 20, 20, 20, 29, 13, 29, 13, 29, 13, 60, 60,
        61, 61, 61, 61, 61, 60, 60, 52, 45, 60, 45, 53, 61, 52, 52, 53, 53, 52,
        52, 61, 53, 52, 60, 53, 52, 60, 60, 60, 60, 37, 30, 22,  7, 31, 30, 30,
         7, 30, 30,  7,  7, 30,  7, 30,  7,  7,  7, 30,  7, 23,  7, 22, 30, 23,
        23, 23, 23, 23, 23, 31, 22, 22, 55, 55, 54, 54, 54, 63, 39, 47, 55, 55,
        63, 63, 54, 54, 62, 62, 62, 62, 62, 54, 62, 62, 62, 62, 63, 63, 63, 63,
        47, 47, 47, 47])
test_res shape torch.Size([256])
test_targets tensor([17,  9,  1, 24,  9, 24, 25, 16, 25,  1, 24, 16, 17,  1, 25, 25, 25,  9,
        17, 25, 24, 25,  1,  9, 16, 25,  1, 25, 17, 25, 25,  1,  3, 27, 18, 11,
        11, 27, 27, 11,  3, 11, 27, 27, 19,  3, 19, 26, 19, 26,  3,  3, 26, 26,
        11,  3, 11, 19, 27, 26, 19,  3, 11, 11, 41, 48, 48, 48, 48, 57, 49, 49,
        48, 57, 48, 57, 33, 41, 56, 56, 57, 41, 33, 41, 33, 56, 48, 33, 49, 41,
        49, 48, 56, 41, 49, 56, 59, 43, 58, 35, 58, 59, 35, 58, 35, 35, 59, 59,
        35, 59, 50, 58, 35, 43, 58, 51, 43, 43, 43, 51, 35, 35, 35, 59, 58, 50,
        50, 59, 13, 13,  5, 29, 29, 29, 29, 20, 28, 21, 29, 21, 21, 28, 21, 28,
        28, 20, 21,  5, 21, 13, 29, 29, 28, 20,  5, 28, 20, 13, 29, 13, 60, 52,
        61, 60, 60, 45, 60, 52, 61, 53, 37, 53, 45, 37, 60, 45, 61, 61, 53, 52,
        53, 52, 52, 52, 52, 61, 53, 60, 45, 60, 37, 60, 30, 22,  7, 22, 22,  7,
        30, 30, 31, 23,  7,  7, 15,  7, 31, 22, 15, 30,  7,  7, 23, 23, 22, 22,
        23, 15, 30, 30, 22, 23, 22, 31, 62, 55, 39, 55, 54, 54, 39, 47, 39, 62,
        55, 63, 63, 54, 55, 62, 62, 62, 62, 54, 39, 54, 62, 55, 62, 47, 63, 63,
        47, 62, 62, 47])
test_targets shape torch.Size([256])
epoch 45 iter 113: train loss 0.73890. lr 1.128467e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:37<00:00,  4.54s/it]
epoch 46 iter 113: train loss 0.72903. lr 4.870152e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:39<00:00,  4.56s/it]
epoch 47 iter 113: train loss 0.87033. lr 5.702668e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 48 iter 113: train loss 0.51412. lr 2.330972e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:40<00:00,  4.57s/it]
epoch 49 iter 113: train loss 0.50658. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:53<00:00,  4.68s/it]
epoch 50 iter 113: train loss 0.45593. lr 2.335764e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:35<00:00,  4.52s/it]
epoch 51 iter 113: train loss 0.68576. lr 5.704798e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.58s/it]
epoch 52 iter 113: train loss 0.60815. lr 4.866306e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:49<00:00,  4.65s/it]
epoch 53 iter 113: train loss 0.48208. lr 1.124628e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:47<00:00,  4.63s/it]
epoch 54 iter 113: train loss 0.44377. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:44<00:00,  4.60s/it]
epoch 55 iter 113: train loss 0.42547. lr 3.675540e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:45<00:00,  4.61s/it]
epoch 56 iter 113: train loss 0.78052. lr 5.999986e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:52<00:00,  4.67s/it]
epoch 57 iter 113: train loss 0.55572. lr 3.657718e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:46<00:00,  4.62s/it]
epoch 58 iter 113: train loss 0.42878. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:57<00:00,  4.72s/it]
epoch 59 iter 113: train loss 0.28712. lr 1.138930e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:01<00:00,  4.75s/it]
epoch 60 iter 113: train loss 0.48007. lr 4.880582e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:03<00:00,  4.76s/it]
epoch 61 iter 113: train loss 0.61009. lr 5.696841e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:02<00:00,  4.76s/it]
epoch 62 iter 113: train loss 0.37329. lr 2.317952e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:55<00:00,  4.70s/it]
epoch 63 iter 113: train loss 0.30708. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:53<00:00,  4.68s/it]
epoch 64 iter 113: train loss 0.39311. lr 2.348802e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:56<00:00,  4.70s/it]
epoch 65 iter 113: train loss 0.50262. lr 5.710552e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:59<00:00,  4.74s/it]
epoch 66 iter 113: train loss 0.48521. lr 4.855825e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:52<00:00,  4.67s/it]
epoch 67 iter 113: train loss 0.29156. lr 1.114217e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:01<00:00,  4.75s/it]
epoch 68 iter 113: train loss 0.27846. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [09:02<00:00,  4.76s/it]
epoch 69 iter 113: train loss 0.37038. lr 3.688553e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:57<00:00,  4.72s/it]
epoch 70 iter 63: train loss 0.49797. lr 5.504516e-04.:  56%|██████████████████████████████▎                       | 64/114 [05:02<04:00,  4.80s/it]testing returns...
test_res tensor([ 1, 51, 31, 58, 43, 29,  3, 35, 35, 21, 29, 28, 31, 31, 19, 33, 21, 11,
        59, 62,  5, 47, 39, 58, 48, 31, 35, 15, 47,  3, 11, 18, 62, 50,  5, 28,
        28, 39, 26, 39, 39, 39, 41, 11, 37, 61, 28, 23, 16, 11, 48, 52, 21, 45,
        54, 24,  1, 41, 39, 37, 49, 35, 50,  7,  5, 56, 30,  1, 55, 17, 60,  5,
        56, 62, 31, 52, 43, 56, 41, 15, 13, 20, 49, 39, 51, 30, 55, 57, 24, 41,
        35, 24, 53, 56, 53, 58, 53, 45, 33, 56, 20,  9, 15, 19, 33, 35, 27, 26,
        48, 17, 60, 43, 23, 35, 30, 29, 31, 53, 58, 11,  5, 61, 27, 43, 16, 58,
        20, 15, 53, 26, 37, 39, 58, 16, 13, 19, 49, 29, 20, 27,  9, 23, 23, 19,
        24,  7, 15,  7, 60, 59, 55, 49, 15, 28, 18, 29, 31,  9, 59, 60, 26, 62,
        20, 17, 17, 17, 35, 57, 62, 47, 47,  7, 27, 43, 28, 30, 13, 45, 22,  3,
        33,  1,  1, 28,  7, 18, 41,  5, 13, 35, 61, 45, 22, 31, 60, 49, 49, 49,
        49, 20, 56, 48, 63, 56, 13, 63, 63, 63, 63, 63, 63, 23, 50, 23, 60, 19,
         7, 11, 23, 23,  9, 41,  9, 39, 39, 31, 31, 17, 17, 17, 60, 17, 53, 53,
        17, 50, 53, 51, 51, 24, 39, 39, 28, 31, 33, 11, 63, 39, 39, 56, 47, 56,
        41, 51, 21, 21])
test_res shape torch.Size([256])
test_targets tensor([57, 51, 31, 58, 43, 29,  3, 17, 35, 21, 29, 13, 41, 31, 19, 33, 21, 11,
        59, 62,  5, 47, 39, 58, 48, 31, 35, 15, 47,  3, 11, 18, 62, 50,  5, 13,
        28, 39, 26, 39, 27, 55, 41, 11, 37, 61, 28, 23, 16, 11, 48, 52, 21, 45,
        54, 24,  1, 41, 39, 37, 49, 35, 50,  1,  5, 56, 30,  1, 55, 17, 60,  5,
        56, 62, 31, 52, 43, 56, 41, 15, 13, 20, 49, 39, 51, 30, 55, 57, 24, 41,
        35, 24, 53, 56, 53, 58, 53,  7, 33, 56, 20,  9, 15, 19, 33, 35, 27, 26,
        48, 17, 60, 43, 23, 35, 30, 29, 31, 53, 24, 11,  5, 61, 27, 43, 16, 58,
        58, 15, 53, 26, 37, 39, 58, 16, 13, 19, 49, 29, 20, 27,  9, 35, 23, 19,
        24,  7, 15,  7, 60, 59, 55, 49, 15, 28, 18, 29, 31,  9, 58, 60, 26, 62,
        20, 51, 43, 17, 35, 57, 62, 47, 47,  7, 27, 43, 28, 30, 13, 45, 22,  3,
        33, 17,  1, 28,  7, 18, 41,  5, 13, 35, 61, 45, 56, 31, 60, 16, 49, 49,
        49, 20, 56, 48, 63, 56, 41, 13, 63, 23, 63, 45, 49, 35, 24, 50, 60, 19,
         7, 11, 54, 23, 31, 41,  9, 39,  7, 31, 19, 43, 17, 17, 60, 24, 49, 53,
        17, 50, 53, 47, 51, 24, 39, 53, 28, 31, 33, 11, 63, 39, 11, 56, 47, 50,
        41, 51, 57, 21])
test_targets shape torch.Size([256])
epoch 70 iter 113: train loss 0.50733. lr 5.999916e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:57<00:00,  4.72s/it]
epoch 71 iter 103: train loss 0.34073. lr 3.984541e-04.:  91%|███████████████████████████████████████████████▍    | 104/114 [08:07<00:47,  4.71s/it]testing returns...
test_res tensor([ 1,  1, 16, 24, 16, 25,  9, 24, 24,  9, 24, 24, 25, 24, 17, 17, 17, 25,
        25,  1, 25, 17, 24, 17, 24, 25,  1, 24, 24, 24, 25, 17,  3,  3,  3, 19,
        11, 11,  3, 26, 19, 11, 18, 27,  3, 27, 18, 26, 18, 11, 19, 19,  3, 11,
         3, 27, 19, 26, 18, 18, 26,  3, 26, 26, 41, 56, 41, 41, 33, 56, 48, 49,
        33, 57, 41, 57, 41, 49, 49, 41, 48, 56, 57, 33, 57, 33, 33, 56, 56, 56,
        56, 33, 56, 56, 56, 48, 43, 58, 50, 51, 51, 58, 50, 58, 58, 58, 58, 59,
        43, 58, 51, 51, 59, 59, 59, 59, 51, 43, 51, 51, 50, 43, 58, 43, 51, 51,
        51, 51, 21, 28, 21, 21,  5,  5,  5, 13,  5,  5, 20,  5, 20, 20,  5,  5,
         5, 20,  5,  5,  5,  5, 13, 21, 29, 29, 29, 21, 21, 21, 21, 52, 52, 60,
        45, 53, 52, 61, 61, 45, 61, 37, 37, 60, 60, 61, 60, 60, 60, 53, 37, 37,
        52, 52, 37, 37, 37, 37, 45, 37, 61, 37, 37, 61, 15, 15, 15, 23, 31, 23,
         7, 22,  7, 22, 22, 22, 22,  7,  7,  7,  7,  7, 22, 22, 22, 22, 22,  7,
        23, 31, 31, 23, 31, 31, 31, 31, 63, 55, 55, 63, 55, 63, 63, 63, 63, 63,
        47, 62, 62, 62, 39, 63, 39, 39, 54, 54, 62, 54, 63, 47, 63, 63, 55, 39,
        62, 63, 63, 62])
test_res shape torch.Size([256])
test_targets tensor([24,  9, 16, 24, 16, 25,  9, 24, 24,  9, 16, 24, 25, 24, 17, 17, 17, 25,
        25,  1, 25, 17, 24, 17, 24, 25,  1, 25, 24, 24, 25, 17,  3,  3, 26, 19,
        11, 11,  3, 26, 19, 11, 18, 27,  3, 27, 18, 26, 18, 11, 19, 19,  3, 11,
        26, 27, 19, 26, 18, 18, 26,  3, 26, 26, 41, 56, 41, 41, 33, 56, 48, 49,
        33, 57, 41, 57, 41, 49, 49, 41, 48, 56, 57, 33, 57, 49, 33, 41, 33, 56,
        57, 33, 56, 49, 56, 48, 43, 58, 50, 51, 51, 51, 50, 50, 58, 43, 58, 59,
        43, 58, 59, 51, 59, 59, 59, 35, 51, 43, 59, 51, 50, 51, 58, 43, 43, 51,
        51, 51, 21, 28, 21, 21,  5, 28,  5, 13, 21,  5, 28,  5, 28, 20, 20,  5,
         5, 28, 20, 21,  5,  5,  5,  5, 13, 29, 20, 21, 29, 21, 21, 28, 52, 60,
        45, 53, 52, 52, 61, 45, 61, 53, 37, 45, 60, 61, 60, 60, 60, 53, 37, 37,
        52, 45, 52, 37, 37, 37, 45, 37, 45, 37, 61, 52, 15, 23, 15, 31, 31, 23,
        23,  7, 22,  7, 30, 22, 22, 22,  7, 15, 23, 23,  7,  7, 22, 22, 15, 31,
        22, 23, 31,  7, 23, 31, 22, 31, 55, 63, 63, 54, 55, 63, 47, 63, 47, 39,
        55, 47, 63, 62, 62, 63, 39, 47, 39, 54, 55, 62, 47, 54, 63, 47, 63, 39,
        55, 63, 39, 62])
test_targets shape torch.Size([256])
epoch 71 iter 113: train loss 0.38323. lr 3.644674e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:55<00:00,  4.70s/it]
epoch 72 iter 113: train loss 0.30957. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:43<00:00,  4.59s/it]
epoch 73 iter 113: train loss 0.28640. lr 1.149429e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:42<00:00,  4.59s/it]
epoch 74 iter 113: train loss 0.38272. lr 4.890975e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:55<00:00,  4.70s/it]
epoch 75 iter 113: train loss 0.37126. lr 5.690961e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:05<00:00,  4.78s/it]
epoch 76 iter 113: train loss 0.30081. lr 2.304946e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:48<00:00,  4.63s/it]
epoch 77 iter 113: train loss 0.20400. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:48<00:00,  4.63s/it]
epoch 78 iter 113: train loss 0.26248. lr 2.361853e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:00<00:00,  4.74s/it]
epoch 79 iter 113: train loss 0.26123. lr 5.716252e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:01<00:00,  4.75s/it]
epoch 80 iter 113: train loss 0.38247. lr 4.845308e-04.: 100%|████████████████████████████████████████████████████| 114/114 [09:00<00:00,  4.74s/it]
epoch 81 iter 113: train loss 0.23107. lr 1.103843e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:50<00:00,  4.66s/it]
epoch 82 iter 113: train loss 0.22795. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:52<00:00,  4.67s/it]
epoch 83 iter 113: train loss 0.24881. lr 3.701552e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:54<00:00,  4.69s/it]
epoch 84 iter 113: train loss 0.39143. lr 5.999786e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:56<00:00,  4.71s/it]
epoch 85 iter 113: train loss 0.26694. lr 3.631617e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:56<00:00,  4.71s/it]
epoch 86 iter 113: train loss 0.18536. lr 6.000000e-05.: 100%|████████████████████████████████████████████████████| 114/114 [08:56<00:00,  4.70s/it]
epoch 87 iter 113: train loss 0.17222. lr 1.159965e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:59<00:00,  4.73s/it]
epoch 88 iter 113: train loss 0.30085. lr 4.901330e-04.: 100%|████████████████████████████████████████████████████| 114/114 [08:57<00:00,  4.72s/it]
  0%|                                                                                                                       | 0/114 [00:00<?, ?it/s]  0%|                                                                                                                       | 0/114 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/home/yrayhan/works/lpmoss/run_dt_place.py", line 277, in <module>
    trainer.train()
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 197, in train
    run_epoch('train', epoch_num=epoch)
  File "/home/yrayhan/works/lpmoss/mingpt/trainer_placement.py", line 123, in run_epoch
    loss.backward()
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/yrayhan/anaconda3/envs/idx_creator/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

(idx_creator) yrayhan@dbserver2:~/works/lpmoss$ python run_dt_place.py
H11, A, C, C1
[29, 29, 29, 28, 56, 25, 5, 54, 63, 47, 43, 31, 28, 61, 35, 33, 33, 7, 39, 49, 49, 20, 19, 35, 1, 1, 58, 54, 3, 13, 54, 54, 53, 7, 49, 7, 7, 33, 33, 33, 49, 7, 7, 50, 33, 13, 9, 9, 9, 59, 27, 26, 59, 27, 11, 5, 58, 55, 58, 47, 20, 20, 13, 28, 41, 41, 52, 1, 41, 29, 18, 41, 29, 5, 3, 41, 53, 28, 28, 60, 50, 60, 60, 30, 41, 47, 1, 55, 48, 48, 56, 29, 50, 43, 49, 49, 50, 47, 47, 56, 47, 35, 56, 30, 48, 48, 56, 43, 43, 48, 30, 13, 30, 53, 30, 30, 28, 53, 56, 60, 53, 60, 50, 57, 53, 45, 48, 60, 45, 57, 11, 45, 45, 57, 26, 57, 26, 57, 26, 45, 1, 57, 26, 52, 13, 13, 9, 15, 1, 9, 18, 59, 45, 59, 50, 9, 26, 18, 18, 19, 19, 19, 19, 25, 19, 52, 63, 63, 63, 63, 63, 61, 59, 55, 55, 25, 55, 27, 54, 11, 54, 55, 37, 24, 37, 43, 43, 22, 21, 22, 15, 22, 15, 21, 15, 22, 22, 62, 5, 22, 3, 21, 21, 24, 20, 20, 20, 21, 21, 23, 23, 23, 23, 58, 16, 37, 39, 62, 62, 62, 62, 62, 51, 3, 3, 52, 52, 23, 23, 52, 15, 3, 15, 37, 59, 37, 5, 31, 37, 5, 31, 31, 31, 31, 39, 17, 25, 17, 27, 16, 25, 17, 25, 39, 16, 16]

